Metadata-Version: 2.4
Name: companion-backend
Version: 0.1.0
Summary: FastAPI backend for the companion project
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.111.0
Requires-Dist: uvicorn[standard]>=0.30.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: google-generativeai>=0.8.3
Provides-Extra: dev
Requires-Dist: pytest>=8.2.0; extra == "dev"
Requires-Dist: httpx>=0.27.0; extra == "dev"
Provides-Extra: voice
Requires-Dist: faster-whisper>=1.0.3; extra == "voice"

# Companion Backend

FastAPI application powering the companion project.

## Setup

```bash
cd backend
pip install -e .[dev]
# optional: install Whisper-based STT support
pip install -e .[voice]
# copy env template and fill in your Gemini key
cp .env.example .env
# then edit .env to set GEMINI_API_KEY
```

## Run locally

```bash
uvicorn app.main:app --reload --port 8000
```

The app auto-loads `.env` at startup.

## API surface

- `GET /health` – liveness probe.
- `POST /api/chat` – accepts `{ "message": "<user text>" }` and returns a Gemini-inspired reply payload:

```jsonc
{
  "reply": "차분하게 다음 단계를 제안해 드릴게요...",
  "emotion": "happy",
  "ttsDurationMs": 2400
}
```

- `POST /api/voice` – accepts multipart form-data with an `audio` blob (webm/wav/etc). The backend transcribes the audio (Whisper if available, fallback otherwise), generates a Gemini response, and returns:

```jsonc
{
  "transcript": "오늘 하루 좀 길었어.",
  "reply": "그런 날엔 창밖 불빛을 잠깐 바라보는 것도 도움이 돼. 어떤 순간이 가장 길게 느껴졌어?",
  "emotion": "sad",
  "ttsDurationMs": 3600
}
```

The `emotion` field feeds the frontend `VideoAvatar`, and `ttsDurationMs` instructs how long the avatar should remain in the `talking` state before falling back to `idle`. The UI also uses the `reply` to synthesize speech via the browser's built-in SpeechSynthesis engine.
